<p>Here I’ll showcase the capabilities of the Jax framework to
solve some interesting problems. We will be implementing the
Gaussian Process-Upper Confidence Bound (GP-UCB) algorithm which is
often used to solve the Bayesian Optimization problem. This is also
a test of the blog’s capabilities!</p>

<p>In the Bayesian Optimization problem, we have some function
<script type="math/tex">f: \mathbb{R}^d \mapsto \mathbb{R}</script>. At each timestep we
choose a point <script type="math/tex">x_t</script> and observe a noisy realization of the
function value <script type="math/tex">y_t \sim f(x_t) + \mathcal{N}(0, \sigma^2)</script>.</p>

<p>We want to minimize the cumulative regret</p>

<script type="math/tex; mode=display">\sum_{t=0}^{T}f(x_t).</script>

<p>The objective means that we are interested in both finding
a global minimum of the <script type="math/tex">f</script>, but we also don’t want to incur a
lot of cost while doing so. Even if we have a pretty good idea of
where the minimum is, we don’t want to choose all our remaining <script type="math/tex">x</script>s
to be there, as we could be missing an even better minimum.</p>

<p>An application of this could be in an idealized medical trial, where
<script type="math/tex">x</script> represents dosage of a drug and <script type="math/tex">f(x)</script> is the patient’s risk of
adverse outcomes. We want to find a dosage which is good, but at the
same time we don’t want to cause many adverse outcomes while doing so.</p>

<p>GP-UCB is a popular method for doing Bayesian Optimization, and it also
has theoretical guarantees upper-bounding the regret (although as is often
the case, these guarantees depend on knowing parameters of <script type="math/tex">f</script> that you’re
unlikely to know a-priori). (Although in this setting we want to minimize
the function instead of maximizing it and so technically we should perhaps
call the algorithm the GP-Lower Confidence Bound, algorithm it seems that
GP-UCB is the name used to describe both the function minimization and
maximization setting).</p>

<p>One advantage of GP-UCB is that it is very straightforward to describe.
Given the points up to the current point, you fit a Gaussian process
to serve as a prediction of the objective function. The Gaussian process
gives us a mean function <script type="math/tex">\mu_{t}(x)</script> and a variance
function <script type="math/tex">\sigma_{t}(x)</script>. GP-UCB chooses the next point as</p>

<script type="math/tex; mode=display">x_{t+1} = \argmin_x\{ \mu_{t}(x) - \beta_t \sigma_{t}(x)\}</script>

<p>where <script type="math/tex">\beta_t</script> is a parameter which is chosen to grow roughly logarithmically
with <script type="math/tex">t</script>. The growth of <script type="math/tex">\beta</script> with <script type="math/tex">t</script> ensures that the algorithm will eventually
sample all the points infinitely often. If the algorithm doesn’t sample in a region
for many iterations, then <script type="math/tex">\sigma</script> will remain the same while <script type="math/tex">\beta</script> slowly grows
until the algorithm samples at that point.</p>

<p>Often this algorithm is implemented by discretizing the input space into a set
of points. In that setting the minimization above simply becomes a linear scan
over the UCB values at all the grid points, and is very easily implemented. The
computation of the GP can be handled by any of many packages (I like Scikit-Learn’s
<code class="highlighter-rouge">GaussianProcessRegressor</code>). But a shortcoming with this approach is that
it scales very poorly with dimension: even using a grid with 10 points per side
would be completely intractable to compute in 15 dimensions.</p>

<p>Instead we can implicitly compute the GP objective function, and
use gradient-based optimization to solve the optimization problem above.
It’d be nice to use more sophisticated optimization techniques (such as Newton’s
method) as they’ll converge much faster. However, computing the gradient of the
GP objective is not completely straightforward, and computing the Hessian is
even more annoying. With the built-in autodiff capabilities of Jax, this becomes
as easy as applying the <code class="highlighter-rouge">grad</code> functional to our gp objective!</p>

<p>For simplicity, we are going to use a domain <script type="math/tex">\mathcal{X} = [0, 1]^d</script>.  This means that we need to use a constrained optimization method
to deal with what happens if the gradient iterates take us out of the
domain. We will use a <a href="https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/15-barr-method-scribed.pdf">log-barrier
method</a>
to solve this constrained optimization problem. Since the gradient
changes very rapidly near the log barrier, it’s important that we use
a second-order method to ensure that the optimization remains
well-conditioned.  We could also have used a <a href="https://ttic.uchicago.edu/~nati/Teaching/TTIC31070/2015/Lecture16.pdf">projected gradient
method</a>,
which would probably have been easier to implement.</p>

<p>Let’s start by defining a function to compute the GP-UCB values.
The Gaussian Process mean function at a predictive
point <script type="math/tex">x_*</script> while conditioning on <script type="math/tex">n</script> points in an <script type="math/tex">n \times d</script>
matrix <script type="math/tex">X</script> is given by</p>

<script type="math/tex; mode=display">\mu(x_*) = K(X_*, X)\left[ K(X, X) + \sigma_n^2I_n\right] ^{-1}y</script>

<p>while the predictive standard deviation is given by</p>

<script type="math/tex; mode=display">K(X_*, X_*)\left[ K(X, X) + \sigma_n^2 I_n \right]^{-1} K(X, X_*).</script>

<p>In both cases, the matrix <script type="math/tex">K(X,Z)</script> is the Gram matrix given by
<script type="math/tex">K_{i,j} = k(x_i, z_j)</script>. We will use the standard choice of a
squared exponential kernel <script type="math/tex">k(x, x') = \exp(-\|x - x'\|^2_2 / \ell^2)</script>
with a given length scale hyperparameter <script type="math/tex">\ell</script></p>

<p>Since we re-use the matrix <script type="math/tex">\left[ K(X, X) + \sigma_n^2I_n \right]^{-1}</script>
and it is independent of <script type="math/tex">X_*</script> we can save time by computing it once and passing
it as an argument to our function. Writing this all out, we have</p>

<p>{% highlight python %}
from <strong>future</strong> import print_function, division
import jax.numpy as jaxnp
import jax.random as jaxrand
import numpy as np
from jax import grad, jit, vmap
import jax.lax as jaxlax</p>

<p>def k(x, length_scale):
    “"”Squared exponential kernel function. 
    Input: difference vector between two points”””
    return jaxnp.exp(-jaxnp.sum(x<strong>2) / length_scale</strong>2)</p>

<p>k_vmap = vmap(k, in_axes=(0, None))</p>

<p>def ucb_vals(x, xs, ys, length_scale, beta, A, interrupt_flag=False):
    “"”Given a kernel k and a set of points xs, and objective values ys, return
    the values of mu(x) and sqrt(beta) * sigma(x), where mu, sigma are the mean and
    variance function of the GP with kernel k conditioned on xs, ys. For
    convenience we pass in the matrix A = (k(xs, xs) + sigma^2 I)^{-1}”””</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># xs = n x d array
# x = 1 x d array
# We assume that k has been vectorized so that it can deal with a whole
# input batch and return batchwise outs
# k is a function b x d -&gt; b, where b is the batch dimension
# here we'll only consider stationary kernels

# If we want to jit functions we can't pass functions (e.g. k) as
# arguments, so we need to set the kernel as a global function
n = xs.shape[0]
d = xs.shape[1]

# See e.g. eq 2.22 in Rasmussen and Williams for details
# mu = k(x, xs) * [k(xs, xs) + sigma^2 I]^{-1} * y
# sigma = k(x,x) - k(x, xs) * [k(xs, xs) + sigma^2 I]^{-1} * k(xs, x)

x_new = jaxnp.reshape(x, (1, d))
k_x_xs = k_vmap(xs - jaxnp.repeat(x_new, n, axis=0), length_scale)
mu_x = jaxnp.dot(jaxnp.dot(k_x_xs, A), ys)

sigma_x = k_vmap(jaxnp.zeros_like(x_new), length_scale)[0] - jaxnp.dot(jaxnp.dot(k_x_xs, A), k_x_xs.T)
return mu_x, jaxnp.sqrt(beta) * jaxnp.sqrt(sigma_x)
</code></pre></div></div>

<p>def ucb_val(x, xs, ys, length_scale, beta, A):
    mu, beta_sigma = ucb_vals(x, xs, ys, length_scale, beta, A)
    return mu - beta_sigma</p>

<p>{% endhighlight %}</p>

<p>As a running example of a tricky global optimization problem, we’ll use
a one-dimensional function with many local minima:</p>

<script type="math/tex; mode=display">f(x) = (ax + b)\cdot \sin (\phi x)</script>

<p><img class="col three" src="/assets/img/objective_fun.pdf" /></p>

<p>Now we check that the GP is fitting the function properly.
It seems like it is (after some tweaking of the length scales of the
GP and the amount of noise). Next we want to implement the
UCB part of the GP-UCB algorithm.</p>

<p>To recap, we want to do the following.</p>

<ul>
  <li>Start at a random point in the space</li>
  <li>Minimize the GP surrogate with Newton’s method and the log-barrier method until we reach a local minimum</li>
  <li>Start the GP surrogate optimization again from a random point</li>
  <li>Choose the next sampled point to be the lowest minimum found so far.</li>
</ul>

<p>First we make a function get the Hessian of an arbitrary function:</p>

<p>{% highlight python %}
def hessian(fun):
    return jit(jacfwd(jacrev(fun)))
{% endhighlight %}</p>

<p>Yep, it’s that simple in Jax. That being done, we write a function to
give us the log-barrier function on the d-dimensional box with our
objective:</p>

<p>{% highlight python %}
def log_barrier_ucb_val(x, t, xs, ys, length_scale, beta, A):
    “"”For use in constrained optimization”””
    return t * ucb_val(x, xs, ys, length_scale, beta, A) + log_barrier(x)
{% endhighlight %}</p>

<p>This has a parameter <script type="math/tex">t</script> which varies how `sharp’ the log barrier is.
A higher <script type="math/tex">t</script> gives a more precise approximation to the hard constraint
that we want to impose, at the expense of a higher gradient making the
optimization process harder. This is demonstrated in this great figure
from Boyd’s book on convex optimization:</p>

<p><img class="col three" src="/assets/img/boyd_log_barrier.png" /></p>

<p>In practice what we will do is find a minimum for a moderate value of
<script type="math/tex">t</script> then increase <script type="math/tex">t</script>, minimize again, etc.</p>

<p>Ok, so we have a working optimization step. Since have already seen
that we are ending up in local minima, we want to be able to run
this optimization starting from many random restarts: at least
on the order of the number of points observed so far. To do this we will
need to speed up the optimization function. With the current
implementation it takes two seconds to do a single run of the optimization.
This isn’t really acceptable for what is inherently just a few 5 by 5 matrix
computations. We should be able to make this much faster. The first
thing to do is to swap in jitted gradient and Hessians. We simply replace
our line above with</p>

<p>{% highlight python %}
barrier_grad_fn = jit(grad(log_barrier_ucb_val))
barrier_hess_fn = jit(hessian(log_barrier_ucb_val))
{% endhighlight %}</p>

<p>This simple swap doubles the speed of our optimization routine: it now
only takes one second to run. We now have to start making harder
optimizations, and venturing into writing our own jitted functions.
Due to the way Jax compiles functions while jitting them, some of our
idiomatic python expressions are not allowed. However, there are
several workarounds. Let’s attempt to jit the <code class="highlighter-rouge">solve_constrained</code>
function.</p>

<p>We immediately get an error: <code class="highlighter-rouge">TypeError: Abstract value passed to </code>bool<code class="highlighter-rouge">, which requires a concrete value</code>.
This is saying that the jax compiler can’t figure out the value of the
<code class="highlighter-rouge">t &gt; t_lim</code> line at compile time. This takes us to the first rule of
jitting functions: any native python <code class="highlighter-rouge">if</code>, <code class="highlighter-rouge">for</code>, or <code class="highlighter-rouge">while</code> loops have
to be able to be figured out at compile time. Confusingly however,
behind the scenes a jitted function is compiled separately for
differently shaped inputs. So it is ok to write e.g. a for-loop that
loops over each dimension of a vector, as the compiler knows at compile
time what the size of the vector. In this case we can just get rid of the
t_lim and hard-code it to 10000 and put it into an explicit for loop.
Trying to jit this, we again get the same error about booleans, which
is in the line search <code class="highlighter-rouge">while</code> condition this time. Since the line search
really has to be formulated with a breaking condition, we need to reformulate it
using the <code class="highlighter-rouge">jax.lax.while_loop</code> construct, which allows us to use while loops in jax.
The LAX that jax compiles to treats the while loop as a single `block’ (no unrolling)
so it is quicker to compile than if we wrote out the whole thing.</p>

<p>Writing everything out, we get this alternative formulation of the
optimization:</p>

<p>{% highlight python %}
def oob_linesearch_continue_predicate(loop_state):
    x, nu, diff, j = loop_state
    barrier_term = log_barrier(x + nu * diff)
    is_infeasible = jaxnp.where(jaxnp.isnan(barrier_term), 1, 0)
    exceeded_steps = jaxnp.where(j &gt; 10, 0, 1)
    return is_infeasible * exceeded_steps == 1</p>

<p>def oob_linesearch_step_fn(loop_state):
    x, nu, diff, j = loop_state
    nu = nu * 0.5
    return x, nu, diff, j + 1</p>

<p>def do_out_of_bounds_linesearch(x, diff):
    nu_0 = 1.
    j = 0
    x_out, nu_out, _, _ = jaxlax.while_loop(oob_linesearch_continue_predicate,
                                            oob_linesearch_step_fn,
                                            (x, nu_0, diff, j))
    return x_out + nu_out * diff</p>

<p>def newton_inner_fn(loop_state):
    x, t, xs, ys, length_scale, beta, A, diff, i = loop_state
    diff = jaxnp.linalg.solve(barrier_hess_fn(x, t, xs, ys, length_scale, beta, A),
                              -barrier_grad_fn(x, t, xs, ys, length_scale, beta, A))
    x_out = do_out_of_bounds_linesearch(x, diff)
    # x_out, diff = take_newton_step(x, t, xs, ys, length_scale, beta, A)
    return (x_out, t, xs, ys, length_scale, beta, A, diff, i + 1)</p>

<p>def newton_inner_continue_predicate(loop_state):
    x, t, xs, ys, length_scale, beta, A, diff, i = loop_state
    inner_tol = 1e-3
    gradient_norm = jaxnp.sum(diff ** 2) ** 0.5
    gradient_small_enough = jaxnp.where(gradient_norm &lt; inner_tol, 0, 1)
    exceeded_iters = jaxnp.where(i &gt; 30, 0, 1)
    return gradient_small_enough * exceeded_iters == 0</p>

<p>def solve_constrained(x0, xs, ys, length_scale, beta, A):
    “"”Solve constrained minimization with the sequential unconstrained
    minimization technique, (i.e. we solve sequences of log-barrier problems
    with increasing barrier sharpness and warm-start from the previous
    solution) and augmented hessian and grad functions. We also
    do a line search to avoid issues with going out of the constraint box.
    In the current set-up we don’t actually ever use this function, but
    it is a template for the more heavily-optimized versions of the function
    which are below and tailored for specific constraints”””
    x = x0
    t = 10
    # Tighten approximation to strict constraints
    for t in jaxnp.logspace(1, 4, 4):
        x_out, _, _, _, _, _, _, _, _ = jaxlax.while_loop(newton_inner_continue_predicate,
                                                          newton_inner_fn,
                                                          (x, t, xs, ys, length_scale, beta, A,
                                                           x * np.inf, 0))
        x = x_out
    return x</p>

<p>{% endhighlight %}</p>

<p>There are several points worth discussing here. We have formulated the
problem as two nested while loops, the inner one repeatedly taking
Newton steps and calling the linesearch, and the second one handling
the linesearch.  In each linesearch we have to wrap the loop state up
into a variable <code class="highlighter-rouge">loop_state</code>. We also have to provide extra
termination conditions if the loops exceed a certain number of
iterations. We use the fact that jax converts bools into integers to
give a termination condition for the linesearch. When we run this, it
takes a few seconds to compile, but once compiled takes 0.02s to run,
a saving of 100x over the non-jitted runtime.  This allows us to do
many repeats, ensuring that we hit the global minimum.</p>

<p>So now we can put everything together and form the full GP-UCB algorithm,
which uses the minimum value found as the the next point to sample.
This looks like this:</p>

<p>{% highlight python %}
def run_gp_ucb(length_scale, sigma=0.1, T=100, num_restarts=20):
    rng = jaxrand.PRNGKey(0)
    # Choose first point randomly
    x0 = np.random.uniform(low=-1., high=1., size=1).reshape((1, 1))
    y0 = vmap_objective(x0, rng.reshape((1,2)))
    xs = jaxnp.zeros((T, 1))
    ys = jaxnp.zeros((T,))</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Keep track of which xs and ys we have sampled so far
xs_log = jaxops.index_update(xs, jaxops.index[0, :], x0)
ys_log = jaxops.index_update(ys, jaxops.index[0], y0.reshape((1)))

fast_solve_constrained = jit(solve_constrained)
fast_objective = jit(ucb_val)
fast_compute_a = jit(compute_A)
for t in range(1, T):
    best_val = jaxnp.inf
    xs = xs_log[:t, :]
    ys = ys_log[:t]
    A = fast_compute_a(xs, sigma, length_scale)

    rng0 = jaxrand.PRNGKey(0)
    x_try = jaxrand.uniform(rng0, shape=(num_restarts,), minval=-1., maxval=1.)
    cur_time = time.time()

    for n in range(num_restarts):
        new_x = fast_solve_constrained(x_try[n].reshape((1,)), xs,
                                       ys, length_scale, 1., A)
        #optim_y, s = fast_objective(new_x, xs, ys, length_scale, 1., A)
        try_val = fast_objective(new_x, xs, ys, length_scale, 1., A)
        if try_val &lt; best_val:
            best_x = new_x
            best_val = try_val

    # Sample at best x val
    y = vmap_objective(best_x, rng0.reshape((1,2)))
    # Update xs, ys
    xs_log = jaxops.index_update(xs_log, jaxops.index[t, :], best_val)
    ys_log = jaxops.index_update(ys_log, jaxops.index[t], y.reshape((1)))
    print("Sampled with t= {}, took {}s".format(t, time.time() - cur_time))
return xs_log, ys_log {% endhighlight %} 
</code></pre></div></div>

<p>The output of this looks like this:</p>

<p><img class="col three" src="/assets/img/final_plot.png" /></p>

<p>It’s a simple matter to change the dimensionality so that we can handle
higher-dimensional problems. We expect to see a much-smaller-than-exponential
increase in the complexity when moving to higher dimensions.</p>

